---
title: "STA 137 Homework 2"
author: "Camden Possinger"
output: pdf_document
date: '2022-04-14'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### 1.19 Suppose that $x_t = \mu + w_t + \theta w_{t-1}$ where $w_t \sim wn(0,\sigma_w^2)$

### (a) Show that the mean function is $E(x_t) = \mu$

\[E(x_t) = E(\mu + w_t + \theta w_{t-1j}) = E(\mu) + E(w_t) + \theta E(w_{t-1}) = \mu \]

### (b) Show that the autocovariance function of $x_t$ is given by $\gamma_x(0) = \sigma_w^2(1+\theta^2)$, $\gamma_x(\pm 1) = \sigma_w^2 \theta$ and $\gamma_x(h) = 0$ otherwise

\[\gamma_x(h) = Cov(x_{t+h}, x_t) = Cov(\mu + w_{t+h} + \theta w_{t+h-1}, \mu + w_t + \theta w_{t-1}) = \]
\[E((\mu + w_{t+h} + \theta w_{t+h-1} - \mu)(\mu + w_t + \theta w_{t-1} - \mu))= E((w_{t+h} + \theta w_{t+h-1})(w_t + \theta w_{t-1})) = \]
\[E[(w_{t+h})(w_t) + (w_{t+h})(\theta w_{t-1}) + (\theta w_{t+h-1})(w_t) + (\theta w_{t+h-1})(\theta w_{t-1})]\]
\[\gamma_x(h) = \begin{cases} \sigma_w^2(1 + \theta^2) & h = 0 \\ \theta \sigma_w^2 & |h| = 1 \\ 0 & otherwise \end{cases}\]

### (c) Show that $x_t$ is stationary for all values of $\theta \in \mathbb{R}$ 

Since $E(x_t)$ is a constant and $\gamma_x(h)$ only depends on $h$ $x_t$ is stationary for all values of $\theta$ 

### (d) Use (1.35) to calculate $var(\bar{x})$ for estimating $\mu$ when (i) $\theta = 1$, (ii) $\theta = 0$, and (iii) $\theta = -1$

\[var(\bar{x}) = var(\frac{1}{n} \sum_{t = 1}^{n} x_t) = \frac{1}{n^2}cov(\sum_{t = 1}^n x_t, \sum_{s = 1}^n x_s) = \]

\begin{center}

Now we can sum the non-zero covariance of $\bar{x}$ using (1.35) 

\end{center}

\[\frac{1}{n^2}(n\gamma_x(0) + (n-1)\gamma_x(-1) + (n-1)\gamma_x(1)) = \]

\[\frac{1}{n^2}(n \sigma_w^2(1+\theta^2) + 2(n-1)\theta \sigma_w^2) \]

\[var(\bar{x}) = \begin{cases} \frac{\sigma_w^2}{n} & \theta = 0 \\ \frac{2\sigma_w^2}{n}(1+\frac{n-1}{n}) & \theta = 1 \\ \frac{2\sigma_w^2}{n}(1-\frac{n-1}{n}) & \theta = -1       \end{cases}    \]

### (e) In time series, the sample size n is typically large, so that $\frac{(n-1)}{n} \approx 1$.With this as a consideration, comment on the results of part (d); in particular, how does the accuracy in the estimate of the mean $\mu$ change for the three different cases?

\[var(\bar{x}) = \begin{cases} \frac{\sigma_w^2}{n} \rightarrow 0 \text{ as } n \rightarrow \infty & \theta = 0 \\ \frac{2\sigma_w^2}{n}(1+\frac{n-1}{n}) \rightarrow 0 \text{ as } n \rightarrow \infty & \theta = 1 \\ \frac{2\sigma_w^2}{n}(1-\frac{n-1}{n}) = 0 \text{ as } n \rightarrow \infty & \theta = -1       \end{cases}    \]

For $\theta = 0$ and $\theta = 1$ $var(\bar{x})$ converges asymptomaically to 0 when n is large while for $\theta = -1$ $var(\bar{x})$ = 0 since $\frac{n-1}{n} \approx 1$ when n is large. 

### 1.21  Simulate a series of $n = 500$ moving average observations as in Example 1.9 and compute the sample ACF, $\rho(h)$, to lag 20. Compare the sample ACF you obtain to the actual ACF, $\rho(h)$.

First let's compute the actual ACF:

\[v_t = \frac{1}{3}(w_{t-1} + w_t + w_{t+1})\]

\[\gamma_v(h) = cov(v_{t+h}, v_t) = cov(\frac{1}{3}(w_{t+h-1} + w_{t+h} + w_{t+h+1}), \frac{1}{3}(w_{t-1} + w_t + w_{t+1})) = \]

\[\frac{1}{9} cov(w_{t+h-1} + w_{t+h} + w_{t+h+1}, w_{t-1} + w_t + w_{t+1}) =  \]

\[\frac{1}{9} E((w_{t+h-1} + w_{t+h} + w_{t+h+1})(w_{t-1} + w_t + w_{t+1})) =  \]

$$
\begin{aligned}
\frac{1}{9}
E[ 
(w_{t+h-1})(w_{t-1}) + (w_{t+h-1})(w_t) + (w_{t+h-1})(w_{t+1}) + \\
(w_{t+h})(w_{t-1}) + (w_{t+h})(w_t) + (w_{t+h})(w_{t+1}) + \\
(w_{t+h+1})(w_{t-1}) + (w_{t+h+1})(w_t) + (w_{t+h+1})(w_{t+1})
] 
\end{aligned}
$$ 

\[\gamma_v(h) = \begin{cases} \frac{1}{3} \sigma_w^2 & h = 0 \\ \frac{2}{9} \sigma_w^2 & |h| = 1 \\ \frac{1}{9} \sigma_w^2 & |h| = 2 \\ 0 & |h| \geq 3 \end{cases}   \]

\[\rho_v(0) = \frac{\gamma_v(0)}{\gamma_v(0)} = \frac{\frac{1}{3} \sigma_w^2}{\frac{1}{3} \sigma_w^2} = 1 \]

\[\rho_v(\pm 1) = \frac{\gamma_v(\pm 1)}{\gamma_v(0)} = \frac{\frac{2}{9} \sigma_w^2}{\frac{1}{3} \sigma_w^2} = \frac{2}{3} \]

\[\rho_v(\pm 2) = \frac{\gamma_v(\pm 2)}{\gamma_v(0)} = \frac{\frac{1}{9} \sigma_w^2}{\frac{1}{3} \sigma_w^2} = \frac{1}{3} \]

\[\rho_v(\pm 3) = \frac{\gamma_v(\pm 3)}{\gamma_v(0)} = \frac{0}{\frac{1}{3} \sigma_w^2} = 0\]



\[\rho_v(h) = \begin{cases} 1 & h = 0 \\ \frac{2}{3} & |h| = 1 \\ \frac{1}{9} & |h| = 2 \\ 0 & |h| \geq 3 \end{cases}   \]


### (a) Compare the sample ACF to the real ACF: 

```{r}
library(magrittr)

(rnorm(502) %>% filter(sides=2, filter=rep(1/3,3))) %>% na.omit %>% acf(lag.max = 20)

```


Here the values greater than 2 are close to 0 but are not exactly 0 due to the noise in the simulated random variables.

### (b) Repeat part (a) using only n = 50. How does changing n affect the results?

```{r}

(rnorm(52) %>% filter(sides=2, filter=rep(1/3,3))) %>% na.omit %>% acf(lag.max = 20)

```

With n = 50 the values greater than 2 are still not exactly 0 and are greater than the values when n = 500. We can see that as we increase the sample size the sample ACF gets closer to the real ACF. We also add two extra random variables to ensure that there are truly 500 and 50 observations respectively.





